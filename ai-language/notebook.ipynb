{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure AI Language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO list \n",
    "deployment, introduction to AI Langauage\n",
    " some text for each solution but keep the code in one cell if possibnle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom text classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversational language understanding \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üïµÔ∏è‚Äç‚ôÇÔ∏è Language detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "        \"In this sample we own a hotel with customers from all around the globe. We want to eventually \"\n",
    "        \"translate these reviews into English so our manager can read them. However, we first need to know which language \"\n",
    "        \"they are in for more accurate translation. This is the step we will be covering in this sample\\n\"\n",
    "    )\n",
    "    # [START detect_language]\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    endpoint = os.environ[\"AZURE_LANGUAGE_ENDPOINT\"]\n",
    "    key = os.environ[\"AZURE_LANGUAGE_KEY\"]\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "    documents = [\n",
    "        \"\"\"\n",
    "        The concierge Paulette was extremely helpful. Sadly when we arrived the elevator was broken, but with Paulette's help we barely noticed this inconvenience.\n",
    "        She arranged for our baggage to be brought up to our room with no extra charge and gave us a free meal to refurbish all of the calories we lost from\n",
    "        walking up the stairs :). Can't say enough good things about my experience!\n",
    "        \"\"\",\n",
    "        \"\"\"\n",
    "        ÊúÄËøëÁî±‰∫éÂ∑•‰ΩúÂéãÂäõÂ§™Â§ßÔºåÊàë‰ª¨ÂÜ≥ÂÆöÂéªÂØåÈÖíÂ∫óÂ∫¶ÂÅá„ÄÇÈÇ£ÂÑøÁöÑÊ∏©Ê≥âÂÆûÂú®Â§™ËàíÊúç‰∫ÜÔºåÊàëË∑üÊàë‰∏àÂ§´ÈÉΩÂÆåÂÖ®ÊÅ¢Â§ç‰∫ÜÂ∑•‰ΩúÂâçÁöÑÈùíÊò•Á≤æÁ•ûÔºÅÂä†Ê≤πÔºÅ\n",
    "        \"\"\"\n",
    "    ]\n",
    "\n",
    "    result = text_analytics_client.detect_language(documents)\n",
    "    reviewed_docs = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    print(\"Let's see what language each review is in!\")\n",
    "\n",
    "    for idx, doc in enumerate(reviewed_docs):\n",
    "        print(\"Review #{} is in '{}', which has ISO639-1 name '{}'\\n\".format(\n",
    "            idx, doc.primary_language.name, doc.primary_language.iso6391_name\n",
    "        ))\n",
    "    # [END detect_language]\n",
    "    print(\n",
    "        \"When actually storing the reviews, we want to map the review to their ISO639-1 name \"\n",
    "        \"so everything is more standardized\"\n",
    "    )\n",
    "\n",
    "    review_to_language = {}\n",
    "    for idx, doc in enumerate(reviewed_docs):\n",
    "        review_to_language[documents[idx]] = doc.primary_language.iso6391_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Comic Sans MS'\">\n",
    "    üóùÔ∏è Key phrase extraction\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Orchestration workflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Comic Sans MS'\">\n",
    "    üÜî Personally Identifiable Information (PII)\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Comic Sans MS'\">\n",
    "    ‚ùì Custom question answering\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Comic Sans MS'\">\n",
    "    üìä Sentiment analysis and opinion mining\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import typing\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "endpoint = os.environ[\"AZURE_LANGUAGE_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_LANGUAGE_KEY\"]\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(key)\n",
    ")\n",
    "\n",
    "print(\"In this sample we will be a hotel owner going through reviews of their hotel to find complaints.\")\n",
    "\n",
    "print(\n",
    "    \"I first found a handful of reviews for my hotel. Let's see what we have to improve.\"\n",
    ")\n",
    "\n",
    "documents = [\n",
    "    \"\"\"\n",
    "    The food and service were unacceptable, but the concierge were nice.\n",
    "    After talking to them about the quality of the food and the process to get room service they refunded\n",
    "    the money we spent at the restaurant and gave us a voucher for near by restaurants.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    The rooms were beautiful. The AC was good and quiet, which was key for us as outside it was 100F and our baby\n",
    "    was getting uncomfortable because of the heat. The breakfast was good too with good options and good servicing times.\n",
    "    The thing we didn't like was that the toilet in our bathroom was smelly. It could have been that the toilet was broken before we arrived.\n",
    "    Either way it was very uncomfortable. Once we notified the staff, they came and cleaned it and left candles.\n",
    "    \"\"\",\n",
    "    \"\"\"\n",
    "    Nice rooms! I had a great unobstructed view of the Microsoft campus but bathrooms were old and the toilet was dirty when we arrived.\n",
    "    It was close to bus stops and groceries stores. If you want to be close to campus I will recommend it, otherwise, might be better to stay in a cleaner one\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "result = text_analytics_client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "doc_result = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "print(\"\\nLet's first see the general sentiment of each of these reviews\")\n",
    "positive_reviews = [doc for doc in doc_result if doc.sentiment == \"positive\"]\n",
    "mixed_reviews = [doc for doc in doc_result if doc.sentiment == \"mixed\"]\n",
    "negative_reviews = [doc for doc in doc_result if doc.sentiment == \"negative\"]\n",
    "print(\"...We have {} positive reviews, {} mixed reviews, and {} negative reviews. \".format(\n",
    "    len(positive_reviews), len(mixed_reviews), len(negative_reviews)\n",
    "))\n",
    "print(\n",
    "    \"\\nSince these reviews seem so mixed, and since I'm interested in finding exactly what it is about my hotel that should be improved, \"\n",
    "    \"let's find the complaints users have about individual aspects of this hotel\"\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"\\nIn order to do that, I'm going to extract targets of a negative sentiment. \"\n",
    "    \"I'm going to map each of these targets to the mined opinion object we get back to aggregate the reviews by target. \"\n",
    ")\n",
    "target_to_complaints: typing.Dict[str, typing.Any] = {}\n",
    "\n",
    "for document in doc_result:\n",
    "    for sentence in document.sentences:\n",
    "        if sentence.mined_opinions:\n",
    "            for mined_opinion in sentence.mined_opinions:\n",
    "                target = mined_opinion.target\n",
    "                if target.sentiment == 'negative':\n",
    "                    target_to_complaints.setdefault(target.text, [])\n",
    "                    target_to_complaints[target.text].append(mined_opinion)\n",
    "\n",
    "print(\"\\nLet's now go through the aspects of our hotel people have complained about and see what users have specifically said\")\n",
    "\n",
    "for target_name, complaints in target_to_complaints.items():\n",
    "    print(\"Users have made {} complaint(s) about '{}', specifically saying that it's '{}'\".format(\n",
    "        len(complaints),\n",
    "        target_name,\n",
    "        \"', '\".join(\n",
    "            [assessment.text for complaint in complaints for assessment in complaint.assessments]\n",
    "        )\n",
    "    ))\n",
    "\n",
    "\n",
    "print(\n",
    "    \"\\n\\nLooking at the breakdown, I can see what aspects of my hotel need improvement, and based off of both the number and \"\n",
    "    \"content of the complaints users have made about my toilets, I need to get that fixed ASAP.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "        \"In this sample we will be combing through reviews customers have left about their\"\n",
    "        \"experience using our skydiving company, Contoso.\"\n",
    "    )\n",
    "    print(\n",
    "        \"We start out with a list of reviews. Let us extract the reviews we are sure are \"\n",
    "        \"positive, so we can display them on our website and get even more customers!\"\n",
    "    )\n",
    "\n",
    "    # [START analyze_sentiment]\n",
    "    import os\n",
    "    from azure.core.credentials import AzureKeyCredential\n",
    "    from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "    endpoint = os.environ[\"AZURE_LANGUAGE_ENDPOINT\"]\n",
    "    key = os.environ[\"AZURE_LANGUAGE_KEY\"]\n",
    "\n",
    "    text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))\n",
    "\n",
    "    documents = [\n",
    "        \"\"\"I had the best day of my life. I decided to go sky-diving and it made me appreciate my whole life so much more.\n",
    "        I developed a deep-connection with my instructor as well, and I feel as if I've made a life-long friend in her.\"\"\",\n",
    "        \"\"\"This was a waste of my time. All of the views on this drop are extremely boring, all I saw was grass. 0/10 would\n",
    "        not recommend to any divers, even first timers.\"\"\",\n",
    "        \"\"\"This was pretty good! The sights were ok, and I had fun with my instructors! Can't complain too much about my experience\"\"\",\n",
    "        \"\"\"I only have one word for my experience: WOW!!! I can't believe I have had such a wonderful skydiving company right\n",
    "        in my backyard this whole time! I will definitely be a repeat customer, and I want to take my grandmother skydiving too,\n",
    "        I know she'll love it!\"\"\"\n",
    "    ]\n",
    "\n",
    "\n",
    "    result = text_analytics_client.analyze_sentiment(documents, show_opinion_mining=True)\n",
    "    docs = [doc for doc in result if not doc.is_error]\n",
    "\n",
    "    print(\"Let's visualize the sentiment of each of these documents\")\n",
    "    for idx, doc in enumerate(docs):\n",
    "        print(f\"Document text: {documents[idx]}\")\n",
    "        print(f\"Overall sentiment: {doc.sentiment}\")\n",
    "    # [END analyze_sentiment]\n",
    "\n",
    "    print(\"Now, let us extract all of the positive reviews\")\n",
    "    positive_reviews = [doc for doc in docs if doc.sentiment == 'positive']\n",
    "\n",
    "    print(\"We want to be very confident that our reviews are positive since we'll be posting them on our website.\")\n",
    "    print(\"We're going to confirm our chosen reviews are positive using two different tests\")\n",
    "\n",
    "    print(\n",
    "        \"First, we are going to check how confident the sentiment analysis model is that a document is positive. \"\n",
    "        \"Let's go with a 90% confidence.\"\n",
    "    )\n",
    "    positive_reviews = [\n",
    "        review for review in positive_reviews\n",
    "        if review.confidence_scores.positive >= 0.9\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        \"Finally, we also want to make sure every sentence is positive so we only showcase our best selves!\"\n",
    "    )\n",
    "    positive_reviews_final = []\n",
    "    for idx, review in enumerate(positive_reviews):\n",
    "        print(f\"Looking at positive review #{idx + 1}\")\n",
    "        any_sentence_not_positive = False\n",
    "        for sentence in review.sentences:\n",
    "            print(\"...Sentence '{}' has sentiment '{}' with confidence scores '{}'\".format(\n",
    "                sentence.text,\n",
    "                sentence.sentiment,\n",
    "                sentence.confidence_scores\n",
    "                )\n",
    "            )\n",
    "            if sentence.sentiment != 'positive':\n",
    "                any_sentence_not_positive = True\n",
    "        if not any_sentence_not_positive:\n",
    "            positive_reviews_final.append(review)\n",
    "\n",
    "    print(\"We now have the final list of positive reviews we are going to display on our website!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Comic Sans MS'\">\n",
    "    üíä Text Analytics for health\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: 'Comic Sans MS'\">\n",
    "    ‚úçÔ∏è Summarization\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting azure-ai-textanalytics\n",
      "  Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl.metadata (82 kB)\n",
      "Collecting azure-core<2.0.0,>=1.24.0 (from azure-ai-textanalytics)\n",
      "  Downloading azure_core-1.32.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting azure-common~=1.1 (from azure-ai-textanalytics)\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting isodate<1.0.0,>=0.6.1 (from azure-ai-textanalytics)\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting typing-extensions>=4.0.1 (from azure-ai-textanalytics)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting requests>=2.21.0 (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics) (1.17.0)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics)\n",
      "  Downloading charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.21.0->azure-core<2.0.0,>=1.24.0->azure-ai-textanalytics)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading azure_ai_textanalytics-5.3.0-py3-none-any.whl (298 kB)\n",
      "Downloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Downloading azure_core-1.32.0-py3-none-any.whl (198 kB)\n",
      "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp312-cp312-macosx_10_13_universal2.whl (196 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: azure-common, urllib3, typing-extensions, isodate, idna, charset-normalizer, certifi, requests, azure-core, azure-ai-textanalytics\n",
      "Successfully installed azure-ai-textanalytics-5.3.0 azure-common-1.1.28 azure-core-1.32.0 certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 isodate-0.7.2 requests-2.32.3 typing-extensions-4.12.2 urllib3-2.3.0\n",
      "Requirement already satisfied: azure-core in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (1.32.0)\n",
      "Requirement already satisfied: requests>=2.21.0 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from azure-core) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from azure-core) (1.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from azure-core) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core) (2025.1.31)\n",
      "Collecting azure-identity\n",
      "  Downloading azure_identity-1.21.0-py3-none-any.whl.metadata (81 kB)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from azure-identity) (1.32.0)\n",
      "Collecting cryptography>=2.5 (from azure-identity)\n",
      "  Downloading cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (5.7 kB)\n",
      "Collecting msal>=1.30.0 (from azure-identity)\n",
      "  Downloading msal-1.32.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting msal-extensions>=1.2.0 (from azure-identity)\n",
      "  Downloading msal_extensions-1.3.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from azure-identity) (4.12.2)\n",
      "Requirement already satisfied: requests>=2.21.0 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from azure-core>=1.31.0->azure-identity) (2.32.3)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from azure-core>=1.31.0->azure-identity) (1.17.0)\n",
      "Collecting cffi>=1.12 (from cryptography>=2.5->azure-identity)\n",
      "  Downloading cffi-1.17.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting PyJWT<3,>=1.0.0 (from PyJWT[crypto]<3,>=1.0.0->msal>=1.30.0->azure-identity)\n",
      "  Using cached PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=2.5->azure-identity)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nicograssetto/Documents/GitHub/azure-notebooks/.venv/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.31.0->azure-identity) (2025.1.31)\n",
      "Downloading azure_identity-1.21.0-py3-none-any.whl (189 kB)\n",
      "Downloading cryptography-44.0.2-cp39-abi3-macosx_10_9_universal2.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m518.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading msal-1.32.0-py3-none-any.whl (114 kB)\n",
      "Downloading msal_extensions-1.3.1-py3-none-any.whl (20 kB)\n",
      "Downloading cffi-1.17.1-cp312-cp312-macosx_11_0_arm64.whl (178 kB)\n",
      "Using cached PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: PyJWT, pycparser, cffi, cryptography, msal, msal-extensions, azure-identity\n",
      "Successfully installed PyJWT-2.10.1 azure-identity-1.21.0 cffi-1.17.1 cryptography-44.0.2 msal-1.32.0 msal-extensions-1.3.1 pycparser-2.22\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-ai-textanalytics\n",
    "!pip install azure-core\n",
    "!pip install azure-identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries abstracted:\n",
      "The Chief Technology Officer of Azure AI Cognitive Services discusses Microsoft's commitment to advancing AI by integrating monolingual text, audio or visual signals, and multilingual capabilities, termed as the XYZ-code. This approach aims to create AI that can better understand humans across different domains and languages. Through their efforts, Microsoft has achieved human-level performance on key benchmarks in speech recognition, machine translation, conversational question answering, reading comprehension, and image captioning. The ultimate goal is to develop pretrained models that can learn from multiple modalities and languages, akin to human learning, and incorporate external knowledge sources for downstream AI tasks. This progress is seen as a stepping stone towards a significant leap in AI capabilities, with a focus on multisensory and multilingual learning. The XYZ-code is central to this vision, promising a more holistic and human-centric AI.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "endpoint = os.environ[\"AZURE_LANGUAGE_ENDPOINT\"]\n",
    "key = os.environ[\"AZURE_LANGUAGE_KEY\"]\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=AzureKeyCredential(key),\n",
    ")\n",
    "\n",
    "document = [\n",
    "    \"At Microsoft, we have been on a quest to advance AI beyond existing techniques, by taking a more holistic, \"\n",
    "    \"human-centric approach to learning and understanding. As Chief Technology Officer of Azure AI Cognitive \"\n",
    "    \"Services, I have been working with a team of amazing scientists and engineers to turn this quest into a \"\n",
    "    \"reality. In my role, I enjoy a unique perspective in viewing the relationship among three attributes of \"\n",
    "    \"human cognition: monolingual text (X), audio or visual sensory signals, (Y) and multilingual (Z). At the \"\n",
    "    \"intersection of all three, there's magic-what we call XYZ-code as illustrated in Figure 1-a joint \"\n",
    "    \"representation to create more powerful AI that can speak, hear, see, and understand humans better. \"\n",
    "    \"We believe XYZ-code will enable us to fulfill our long-term vision: cross-domain transfer learning, \"\n",
    "    \"spanning modalities and languages. The goal is to have pretrained models that can jointly learn \"\n",
    "    \"representations to support a broad range of downstream AI tasks, much in the way humans do today. \"\n",
    "    \"Over the past five years, we have achieved human performance on benchmarks in conversational speech \"\n",
    "    \"recognition, machine translation, conversational question answering, machine reading comprehension, \"\n",
    "    \"and image captioning. These five breakthroughs provided us with strong signals toward our more ambitious \"\n",
    "    \"aspiration to produce a leap in AI capabilities, achieving multisensory and multilingual learning that \"\n",
    "    \"is closer in line with how humans learn and understand. I believe the joint XYZ-code is a foundational \"\n",
    "    \"component of this aspiration, if grounded with external knowledge sources in the downstream AI tasks.\"\n",
    "]\n",
    "\n",
    "poller = text_analytics_client.begin_abstract_summary(document)\n",
    "abstract_summary_results = poller.result()\n",
    "for result in abstract_summary_results:\n",
    "    if result.kind == \"AbstractiveSummarization\":\n",
    "        print(\"Summaries abstracted:\")\n",
    "        [print(f\"{summary.text}\\n\") for summary in result.summaries]\n",
    "    elif result.is_error is True:\n",
    "        print(\"...Is an error with code '{}' and message '{}'\".format(\n",
    "            result.error.code, result.error.message\n",
    "        ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
